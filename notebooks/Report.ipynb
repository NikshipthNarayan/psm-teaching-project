{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aec3b88-63f0-490d-98d6-d5063fb75d76",
   "metadata": {},
   "source": [
    "# Pedagogical Report: Teaching Propensity Score Matching\n",
    "\n",
    "**INFO 7390: Advanced Data Science and Architecture**\n",
    "\n",
    "**Author:** Nikshipth Narayan Bondugula\n",
    "\n",
    "**Date:** December 2025\n",
    "\n",
    "**Topic:** Propensity Score Matching for Causal Inference\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Teaching Philosophy\n",
    "2. Concept Deep Dive\n",
    "3. Implementation Analysis\n",
    "4. Assessment & Effectiveness\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Teaching Philosophy\n",
    "\n",
    "### 1.1 Target Audience\n",
    "\n",
    "This tutorial is designed for **graduate students in data science, statistics, or related quantitative fields** who have foundational knowledge but are new to causal inference methods.\n",
    "\n",
    "**Assumed Background:**\n",
    "\n",
    "- Proficiency in Python programming and pandas library\n",
    "- Understanding of logistic regression and probability concepts\n",
    "- Familiarity with basic statistical inference (hypothesis testing, confidence intervals)\n",
    "- Experience with data visualization using matplotlib/seaborn\n",
    "- No prior exposure to causal inference or propensity score methods required\n",
    "\n",
    "**Audience Personas:**\n",
    "\n",
    "| Persona | Background | Goals |\n",
    "|---------|------------|-------|\n",
    "| Data Science Student | Strong in ML, weak in causal methods | Learn when correlation ≠ causation |\n",
    "| Healthcare Analyst | Domain expertise, moderate coding | Evaluate treatment effectiveness |\n",
    "| Policy Researcher | Statistics background | Assess program impact from observational data |\n",
    "\n",
    "### 1.2 Learning Objectives\n",
    "\n",
    "By completing this tutorial, students will be able to:\n",
    "\n",
    "**Knowledge (Conceptual Understanding):**\n",
    "\n",
    "1. Explain the fundamental problem of causal inference and why naive comparisons fail\n",
    "2. Define propensity scores and articulate the theoretical basis for their use\n",
    "3. Identify the three critical assumptions (unconfoundedness, positivity, SUTVA) and their implications\n",
    "4. Distinguish between ATE and ATT and know when each is appropriate\n",
    "\n",
    "**Skills (Practical Application):**\n",
    "\n",
    "1. Implement a complete PSM pipeline in Python from scratch\n",
    "2. Estimate propensity scores using logistic regression\n",
    "3. Perform nearest neighbor matching with appropriate caliper selection\n",
    "4. Assess covariate balance using SMD and Love plots\n",
    "5. Calculate treatment effects with confidence intervals\n",
    "\n",
    "**Critical Thinking (Evaluation):**\n",
    "\n",
    "1. Evaluate when PSM is appropriate versus alternative methods\n",
    "2. Identify potential unmeasured confounders in real-world scenarios\n",
    "3. Apply computational skepticism to causal claims from observational studies\n",
    "4. Recognize limitations and communicate uncertainty appropriately\n",
    "\n",
    "### 1.3 Pedagogical Approach and Rationale\n",
    "\n",
    "**Core Philosophy: \"Learn by Teaching, Understand by Doing\"**\n",
    "\n",
    "This tutorial employs a constructivist learning approach where students actively build understanding through hands-on implementation rather than passive absorption of theory.\n",
    "\n",
    "**Instructional Design Principles:**\n",
    "\n",
    "**1. Scaffolded Learning Progression**\n",
    "\n",
    "The tutorial follows a carefully sequenced structure:\n",
    "\n",
    "- **Motivation First:** Begin with a compelling real-world problem (cardiac rehab effectiveness) before introducing technical concepts\n",
    "- **Concept → Math → Code:** Each topic progresses from intuition to formalization to implementation\n",
    "- **Progressive Complexity:** Start with simple visualizations, advance to complete pipeline implementation\n",
    "\n",
    "**2. Multiple Representations**\n",
    "\n",
    "Each concept is presented through multiple modalities:\n",
    "\n",
    "- **Verbal:** Written explanations with analogies and plain language\n",
    "- **Mathematical:** Formal notation and equations for precision\n",
    "- **Visual:** Diagrams, flowcharts, and data visualizations\n",
    "- **Computational:** Working Python code with extensive comments\n",
    "\n",
    "This multi-modal approach accommodates diverse learning styles and reinforces understanding through repetition in different forms.\n",
    "\n",
    "**3. Active Learning Through Exercises**\n",
    "\n",
    "The tutorial includes three levels of exercises:\n",
    "\n",
    "- **Beginner (Conceptual):** Identify confounders and bias direction\n",
    "- **Intermediate (Implementation):** Complete a PSM pipeline on new data\n",
    "- **Advanced (Critical Thinking):** Evaluate real-world study limitations\n",
    "\n",
    "Additionally, \"Try It Yourself\" challenges encourage experimentation with parameters.\n",
    "\n",
    "**4. Worked Examples with Visible Thinking**\n",
    "\n",
    "All code includes extensive comments explaining not just \"what\" but \"why\":\n",
    "\n",
    "- Design decisions are explicitly discussed\n",
    "- Trade-offs are acknowledged\n",
    "- Common mistakes are highlighted proactively\n",
    "\n",
    "**5. Immediate Feedback Loops**\n",
    "\n",
    "- Expected outputs are provided so students can verify correctness\n",
    "- Diagnostic functions help identify problems\n",
    "- The debugging guide addresses common errors\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Concept Deep Dive\n",
    "\n",
    "### 2.1 Technical and Mathematical Foundations\n",
    "\n",
    "**The Fundamental Problem of Causal Inference**\n",
    "\n",
    "At the heart of causal inference lies a fundamental impossibility: we can never observe both potential outcomes for the same individual. For any patient i, we define:\n",
    "\n",
    "- Y_i(1): The outcome if patient i receives treatment\n",
    "- Y_i(0): The outcome if patient i does not receive treatment\n",
    "\n",
    "The individual treatment effect τ_i = Y_i(1) - Y_i(0) is inherently unobservable because each patient either receives treatment or doesn't—never both.\n",
    "\n",
    "**The Propensity Score Solution**\n",
    "\n",
    "Rosenbaum and Rubin (1983) proved a remarkable result: if treatment assignment is strongly ignorable given covariates X, it remains strongly ignorable given only the propensity score e(X) = P(T=1|X). This dimensionality reduction is powerful—instead of matching on potentially dozens of covariates, we match on a single scalar.\n",
    "\n",
    "**Mathematical Framework:**\n",
    "\n",
    "The propensity score is estimated via logistic regression:\n",
    "\n",
    "```\n",
    "log(e(X)/(1-e(X))) = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ\n",
    "```\n",
    "\n",
    "The key assumptions are:\n",
    "\n",
    "1. **Unconfoundedness:** Y(0), Y(1) ⊥ T | X\n",
    "   - Potential outcomes are independent of treatment given covariates\n",
    "   - This is untestable and requires domain knowledge\n",
    "\n",
    "2. **Positivity:** 0 < P(T=1|X) < 1 for all X\n",
    "   - Every covariate combination has some probability of each treatment\n",
    "   - Testable by examining propensity score distributions\n",
    "\n",
    "3. **SUTVA:** No interference between units; treatment is well-defined\n",
    "   - One person's treatment doesn't affect another's outcome\n",
    "   - Requires careful consideration of the study context\n",
    "\n",
    "**The Balancing Property:**\n",
    "\n",
    "A crucial insight is that the propensity score is a balancing score: X ⊥ T | e(X). Among individuals with the same propensity score, the distribution of covariates is the same for treated and control groups. This property enables us to remove confounding by matching on propensity scores.\n",
    "\n",
    "### 2.2 Connection to Course Themes\n",
    "\n",
    "**GIGO (Garbage In, Garbage Out)**\n",
    "\n",
    "The GIGO principle applies powerfully to PSM:\n",
    "\n",
    "- **Garbage Confounders → Garbage Estimates:** If we fail to include important confounders in our propensity model, the resulting matches will not be truly comparable, and our causal estimates will be biased\n",
    "- **Garbage Data Quality → Garbage Propensity Scores:** Missing data, measurement error, or coding mistakes in covariates propagate through to poor propensity estimates\n",
    "- **No Statistical Fix for Missing Variables:** Unlike prediction tasks where we can sometimes compensate for missing features, causal inference fundamentally requires measuring the right confounders\n",
    "\n",
    "The tutorial emphasizes this through:\n",
    "\n",
    "- Explicit discussion of unmeasured confounding\n",
    "- Sensitivity analysis showing how omitting a confounder biases results\n",
    "- \"Break the Method\" exercises demonstrating failure modes\n",
    "\n",
    "**Computational Skepticism**\n",
    "\n",
    "PSM embodies computational skepticism by forcing us to question causal claims:\n",
    "\n",
    "- **Question Assumptions:** The unconfoundedness assumption cannot be tested—we must reason about what might be missing\n",
    "- **Verify Before Trusting:** Balance checks (SMD, Love plots) verify that matching actually worked\n",
    "- **Understand Limitations:** Even well-executed PSM can't prove causation; it only removes bias from measured confounders\n",
    "\n",
    "The tutorial cultivates skepticism through:\n",
    "\n",
    "- Comparing naive estimates to PSM estimates to true effects\n",
    "- Showing how easy it is to get wrong answers\n",
    "- Critical thinking exercises about real-world studies\n",
    "\n",
    "**Botspeak Framework**\n",
    "\n",
    "The tutorial leverages AI collaboration principles:\n",
    "\n",
    "- **Clear Communication:** Technical concepts are explained in plain language first\n",
    "- **Structured Prompts:** Code cells are organized with clear inputs, processes, and outputs\n",
    "- **Iterative Refinement:** The debugging guide helps students iterate toward correct solutions\n",
    "\n",
    "### 2.3 Relationship to Real-World Data Science Workflows\n",
    "\n",
    "**Where PSM Fits in the Data Science Pipeline:**\n",
    "\n",
    "1. **Problem Formulation:** Define causal question, identify treatment and outcome\n",
    "2. **Data Collection:** Gather observational data with relevant confounders\n",
    "3. **Exploratory Analysis:** Visualize selection bias, check data quality\n",
    "4. **PSM Analysis:** Estimate propensity scores, match, assess balance\n",
    "5. **Effect Estimation:** Calculate ATT/ATE with uncertainty quantification\n",
    "6. **Sensitivity Analysis:** Assess robustness to assumptions\n",
    "7. **Communication:** Report findings with appropriate caveats\n",
    "\n",
    "**Industry Applications:**\n",
    "\n",
    "| Domain | Example Application |\n",
    "|--------|---------------------|\n",
    "| Healthcare | Evaluating treatment effectiveness from EHR data |\n",
    "| Marketing | Measuring campaign ROI without A/B testing |\n",
    "| Public Policy | Assessing job training program impact |\n",
    "| Technology | Estimating feature impact from observational logs |\n",
    "| Finance | Evaluating intervention effects on customer behavior |\n",
    "\n",
    "**Why This Matters for Data Scientists:**\n",
    "\n",
    "Many real-world questions are causal, but randomized experiments are often impossible due to:\n",
    "\n",
    "- Ethical constraints (can't randomly deny beneficial treatment)\n",
    "- Practical limitations (too expensive, too slow)\n",
    "- Retrospective analysis needs (data already collected)\n",
    "\n",
    "PSM provides a principled approach to extract causal insights from observational data—a critical skill as organizations increasingly rely on data-driven decision making.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implementation Analysis\n",
    "\n",
    "### 3.1 Architecture and Design Decisions\n",
    "\n",
    "**Modular Function Design**\n",
    "\n",
    "The implementation follows software engineering best practices with modular, reusable functions:\n",
    "\n",
    "```\n",
    "estimate_propensity_scores()  →  Returns: scores, model, scaler\n",
    "nearest_neighbor_matching()   →  Returns: matched_df, match_info\n",
    "calculate_smd()               →  Returns: standardized mean difference\n",
    "assess_balance()              →  Returns: balance DataFrame\n",
    "estimate_att()                →  Returns: results dictionary\n",
    "create_love_plot()            →  Returns: matplotlib figure\n",
    "```\n",
    "\n",
    "**Design Rationale:**\n",
    "\n",
    "- **Single Responsibility:** Each function does one thing well\n",
    "- **Clear Interfaces:** Explicit inputs and outputs with docstrings\n",
    "- **Composability:** Functions can be chained together or used independently\n",
    "- **Testability:** Isolated functions are easier to debug and verify\n",
    "\n",
    "**Data Flow Architecture:**\n",
    "\n",
    "```\n",
    "Raw Data → Propensity Estimation → Matching → Balance Check → Effect Estimation\n",
    "    ↓              ↓                   ↓            ↓               ↓\n",
    " DataFrame    PS Column Added    Matched DF    SMD Table      ATT + CI\n",
    "```\n",
    "\n",
    "**Key Implementation Choices:**\n",
    "\n",
    "| Decision | Choice Made | Rationale |\n",
    "|----------|-------------|-----------|\n",
    "| PS Estimation | Logistic Regression | Standard, interpretable, well-understood |\n",
    "| Feature Scaling | StandardScaler | Improves convergence, required for regularization |\n",
    "| Matching Algorithm | Nearest Neighbor | Intuitive, widely used, good baseline |\n",
    "| Matching Direction | Treated → Control | Estimates ATT (most common estimand) |\n",
    "| Default Caliper | 0.2 × SD | Industry standard, balances match quality and quantity |\n",
    "| Replacement | Without (default) | Simpler variance estimation for teaching |\n",
    "\n",
    "### 3.2 Libraries and Tools\n",
    "\n",
    "**Core Dependencies:**\n",
    "\n",
    "| Library | Version | Purpose |\n",
    "|---------|---------|---------|\n",
    "| numpy | ≥1.20 | Numerical computations |\n",
    "| pandas | ≥1.3 | Data manipulation |\n",
    "| matplotlib | ≥3.4 | Static visualizations |\n",
    "| seaborn | ≥0.11 | Statistical visualizations |\n",
    "| scikit-learn | ≥0.24 | Logistic regression, scaling |\n",
    "| scipy | ≥1.7 | Statistical tests |\n",
    "\n",
    "**Why These Choices:**\n",
    "\n",
    "- **Ubiquity:** All libraries are standard in data science curricula\n",
    "- **Stability:** Mature, well-maintained packages with excellent documentation\n",
    "- **Pedagogical Value:** Students likely already know these tools\n",
    "- **No Black Boxes:** Using sklearn's LogisticRegression rather than specialized PSM packages keeps the implementation transparent\n",
    "\n",
    "**Alternative Libraries (Mentioned for Reference):**\n",
    "\n",
    "For production use, students are pointed to specialized packages:\n",
    "\n",
    "- `pymatch`: Full PSM pipeline with additional features\n",
    "- `causalinference`: Comprehensive causal inference toolkit\n",
    "- `DoWhy`: Microsoft's causal reasoning library\n",
    "- `EconML`: Machine learning for causal inference\n",
    "\n",
    "### 3.3 Performance Considerations\n",
    "\n",
    "**Computational Complexity:**\n",
    "\n",
    "| Step | Complexity | Bottleneck |\n",
    "|------|------------|------------|\n",
    "| PS Estimation | O(n × k × iterations) | Logistic regression convergence |\n",
    "| Nearest Neighbor Matching | O(n_treated × n_control) | Distance calculations |\n",
    "| Balance Assessment | O(k × n) | SMD calculations per covariate |\n",
    "\n",
    "**Scalability Discussion:**\n",
    "\n",
    "The naive matching implementation has O(n²) complexity in the worst case. For the tutorial's 2,000 patients, this is instantaneous. For larger datasets:\n",
    "\n",
    "- **10,000 patients:** ~1-2 seconds\n",
    "- **100,000 patients:** May require optimization\n",
    "- **1,000,000+ patients:** Need approximate nearest neighbor methods (KD-trees, Ball trees)\n",
    "\n",
    "The tutorial acknowledges this limitation and points students to optimized libraries for production use.\n",
    "\n",
    "**Memory Considerations:**\n",
    "\n",
    "The implementation creates copies of dataframes for clarity. For very large datasets, in-place operations or chunked processing would be necessary.\n",
    "\n",
    "### 3.4 Edge Cases and Limitations\n",
    "\n",
    "**Handled Edge Cases:**\n",
    "\n",
    "| Edge Case | How Addressed |\n",
    "|-----------|---------------|\n",
    "| No available controls | Skip treated unit, count as unmatched |\n",
    "| Perfect separation in PS | Warning in diagnostic function |\n",
    "| Zero variance covariate | Return SMD = 0 |\n",
    "| Empty match result | Return empty DataFrame with info |\n",
    "\n",
    "**Known Limitations:**\n",
    "\n",
    "1. **No Support for Continuous Treatments:** Implementation assumes binary treatment only\n",
    "\n",
    "2. **Simple Variance Estimation:** The SE calculation assumes independent observations; doesn't account for matching with replacement\n",
    "\n",
    "3. **Single Matching Algorithm:** Only nearest neighbor implemented; production systems should offer alternatives (optimal matching, genetic matching)\n",
    "\n",
    "4. **No Automatic Covariate Selection:** Users must specify confounders based on domain knowledge\n",
    "\n",
    "5. **Limited Sensitivity Analysis:** Conceptual discussion of Rosenbaum bounds but no full implementation\n",
    "\n",
    "**Pedagogical Note:** These limitations are intentional—a \"perfect\" implementation would obscure the core concepts. Students are encouraged to explore specialized packages for production work.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Assessment & Effectiveness\n",
    "\n",
    "### 4.1 Validating Student Understanding\n",
    "\n",
    "**Formative Assessment (During Learning):**\n",
    "\n",
    "1. **Expected Outputs:** Each code section includes expected outputs so students can self-verify\n",
    "   - Propensity score ranges\n",
    "   - Number of matched pairs\n",
    "   - SMD values before/after matching\n",
    "   - ATT estimate magnitude\n",
    "\n",
    "2. **Reflection Questions:** Embedded questions prompt metacognition\n",
    "   - \"Why is the naive estimate biased toward zero?\"\n",
    "   - \"What would happen with unmeasured confounders?\"\n",
    "\n",
    "3. **Diagnostic Function:** `psm_diagnostic_report()` provides automated feedback on common issues\n",
    "\n",
    "**Summative Assessment (After Learning):**\n",
    "\n",
    "Three levels of exercises assess different cognitive depths:\n",
    "\n",
    "| Level | Type | Assesses |\n",
    "|-------|------|----------|\n",
    "| Beginner | Conceptual questions | Understanding of selection bias, confounding |\n",
    "| Intermediate | Implementation task | Ability to apply PSM to new data |\n",
    "| Advanced | Critical analysis | Evaluation of real-world study limitations |\n",
    "\n",
    "**Rubric for Exercise Evaluation:**\n",
    "\n",
    "| Criterion | Excellent | Satisfactory | Needs Work |\n",
    "|-----------|-----------|--------------|------------|\n",
    "| Conceptual Understanding | Correctly identifies all confounders and bias direction | Identifies most confounders | Confuses correlation with causation |\n",
    "| Implementation | Code runs correctly, matches expected output | Minor errors, mostly correct | Significant errors or incomplete |\n",
    "| Critical Thinking | Identifies multiple limitations, proposes solutions | Identifies obvious limitations | Accepts results uncritically |\n",
    "\n",
    "### 4.2 Common Challenges Students May Face\n",
    "\n",
    "**Conceptual Challenges:**\n",
    "\n",
    "| Challenge | How Tutorial Addresses It |\n",
    "|-----------|---------------------------|\n",
    "| Confusing prediction with causal inference | Explicit section on \"Why Simple Comparison Fails\" |\n",
    "| Misunderstanding propensity score meaning | Multiple explanations: verbal, mathematical, visual |\n",
    "| Assuming PSM \"proves\" causation | Repeated emphasis on assumptions and limitations |\n",
    "| Difficulty with potential outcomes framework | Concrete examples with patient scenarios |\n",
    "\n",
    "**Technical Challenges:**\n",
    "\n",
    "| Challenge | How Tutorial Addresses It |\n",
    "|-----------|---------------------------|\n",
    "| Logistic regression convergence issues | StandardScaler applied; debugging guide covers this |\n",
    "| Poor overlap/no matches | Visualization of PS distributions; diagnostic checks |\n",
    "| Interpreting SMD values | Clear thresholds (0.1, 0.25) with visual guides |\n",
    "| Understanding caliper selection | Explanation of 0.2 × SD convention; sensitivity analysis |\n",
    "\n",
    "**Common Mistakes Addressed:**\n",
    "\n",
    "The dedicated \"Common Mistakes & Debugging Tips\" section covers:\n",
    "\n",
    "1. Including post-treatment variables\n",
    "2. Propensity scores at 0 or 1\n",
    "3. Ignoring balance checks\n",
    "4. Wrong caliper scale\n",
    "5. Forgetting to set random seed\n",
    "\n",
    "### 4.3 Addressing Different Learning Styles\n",
    "\n",
    "**Visual Learners:**\n",
    "\n",
    "- Extensive use of diagrams (DAGs, flowcharts, ASCII art)\n",
    "- Multiple visualization types (histograms, Love plots, bar charts)\n",
    "- Color-coded outputs (✅ ⚠️ ❌)\n",
    "\n",
    "**Reading/Writing Learners:**\n",
    "\n",
    "- Comprehensive markdown documentation\n",
    "- Detailed comments in code\n",
    "- Summary tables throughout\n",
    "\n",
    "**Auditory Learners:**\n",
    "\n",
    "- Companion video follows Explain → Show → Try structure\n",
    "- Verbal walkthrough of concepts before code\n",
    "\n",
    "**Kinesthetic Learners:**\n",
    "\n",
    "- Hands-on implementation from scratch\n",
    "- \"Try It Yourself\" challenges\n",
    "- Starter template with TODOs to complete\n",
    "\n",
    "### 4.4 Future Improvements and Extensions\n",
    "\n",
    "**Planned Enhancements:**\n",
    "\n",
    "1. **Interactive Widgets:** Add ipywidgets for real-time parameter exploration\n",
    "   - Slider for caliper adjustment\n",
    "   - Dropdown for matching algorithm selection\n",
    "   - Live-updating balance plots\n",
    "\n",
    "2. **Additional Matching Methods:**\n",
    "   - Optimal matching\n",
    "   - Coarsened exact matching\n",
    "   - Genetic matching\n",
    "\n",
    "3. **Full Sensitivity Analysis:**\n",
    "   - Implement Rosenbaum bounds\n",
    "   - E-value calculations\n",
    "\n",
    "4. **Real-World Datasets:**\n",
    "   - Add examples with messy, realistic data\n",
    "   - Include missing data handling\n",
    "\n",
    "5. **Integration with Causal Graphs:**\n",
    "   - DAG-based confounder selection\n",
    "   - Connection to do-calculus\n",
    "\n",
    "**Student-Suggested Extensions:**\n",
    "\n",
    "Based on anticipated feedback, potential additions include:\n",
    "\n",
    "- Video walkthroughs for each section\n",
    "- Interactive quiz questions\n",
    "- Comparison with other causal methods (IV, DiD, RDD)\n",
    "- Case studies from published research\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. *Biometrika, 70*(1), 41-55.\n",
    "\n",
    "2. Austin, P. C. (2011). An introduction to propensity score methods for reducing the effects of confounding in observational studies. *Multivariate Behavioral Research, 46*(3), 399-424.\n",
    "\n",
    "3. Caliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. *Journal of Economic Surveys, 22*(1), 31-72.\n",
    "\n",
    "4. King, G., & Nielsen, R. (2019). Why propensity scores should not be used for matching. *Political Analysis, 27*(4), 435-454.\n",
    "\n",
    "5. Cunningham, S. (2021). *Causal Inference: The Mixtape*. Yale University Press.\n",
    "\n",
    "6. Huntington-Klein, N. (2021). *The Effect: An Introduction to Research Design and Causality*. Chapman and Hall/CRC.\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: AI Assistance Acknowledgment\n",
    "\n",
    "This pedagogical report and associated tutorial materials were developed with assistance from Claude (Anthropic) for:\n",
    "\n",
    "- Code debugging and optimization\n",
    "- Generating synthetic datasets\n",
    "- Creating explanatory diagrams and flowcharts\n",
    "- Proofreading and formatting\n",
    "- Structuring pedagogical content\n",
    "\n",
    "All pedagogical approaches, learning objectives, and educational design decisions represent original work by the author. The use of AI assistance enhanced productivity while maintaining academic integrity through transparent acknowledgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c96353-368f-46b2-9f65-6b08c5f2acf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
