{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec52f46-be2c-4452-bc32-0fcfad54ccc2",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Propensity Score Matching: Starter Template\n",
    "## Hands-On Practice Notebook\n",
    "\n",
    "**INFO 7390: Advanced Data Science and Architecture**  \n",
    "**Author:** Nikshipth Narayan Bondugula  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Your Mission\n",
    "\n",
    "In this notebook, you will implement a **Propensity Score Matching (PSM)** pipeline from scratch to estimate the causal effect of a cardiac rehabilitation program on hospital readmission.\n",
    "\n",
    "**You will:**\n",
    "1. Explore the data and identify selection bias\n",
    "2. Estimate propensity scores using logistic regression\n",
    "3. Perform nearest neighbor matching\n",
    "4. Assess covariate balance\n",
    "5. Estimate the Average Treatment Effect on the Treated (ATT)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Instructions\n",
    "\n",
    "- Look for `# TODO:` comments â€” these are sections YOU need to complete\n",
    "- Hints are provided to guide you\n",
    "- Expected outputs are shown so you can verify your work\n",
    "- If you get stuck, refer to the complete tutorial notebook\n",
    "\n",
    "---\n",
    "\n",
    "## Difficulty Level: â­â­â­ Intermediate\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c24bd-09a0-4153-8b30-2725eee3f876",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Imports\n",
    "\n",
    "Run this cell to import all required libraries. **No changes needed here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad55d9a-1e6f-4ce6-b831-0ce1e9c273ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP: Import Required Libraries (No changes needed)\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133fad50-e211-4233-935a-73a55b1cc3d2",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Data\n",
    "\n",
    "We'll use a synthetic dataset about a **cardiac rehabilitation program**.\n",
    "\n",
    "**Research Question:** Does participating in cardiac rehab reduce 30-day hospital readmission?\n",
    "\n",
    "**The Challenge:** Patients self-select into the program, creating selection bias.\n",
    "\n",
    "### Variables in the Dataset:\n",
    "\n",
    "| Variable | Description | Role |\n",
    "|----------|-------------|------|\n",
    "| `age` | Patient age (45-85) | Confounder |\n",
    "| `severity` | Disease severity (1-10) | Confounder |\n",
    "| `prior_admits` | Previous hospitalizations (0-5) | Confounder |\n",
    "| `insurance` | Has premium insurance (0/1) | Confounder |\n",
    "| `motivation` | Health motivation score (1-10) | Confounder |\n",
    "| `treatment` | Enrolled in cardiac rehab (0/1) | Treatment |\n",
    "| `readmitted` | Readmitted within 30 days (0/1) | Outcome |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8374a5c7-1471-4ee3-b25c-83034279dba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset generated!\n",
      "   Shape: 2000 patients, 11 variables\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 1A: Data Generation Function (No changes needed - just run this)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_cardiac_rehab_data(n_patients=2000, true_treatment_effect=-0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic cardiac rehabilitation dataset with realistic confounding.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate confounders\n",
    "    age = np.random.normal(65, 10, n_patients).clip(45, 85)\n",
    "    severity = np.random.normal(5, 2, n_patients).clip(1, 10)\n",
    "    prior_admits = np.random.poisson(1.5, n_patients).clip(0, 5)\n",
    "    insurance = np.random.binomial(1, 0.4, n_patients)\n",
    "    motivation = np.random.normal(5, 2, n_patients).clip(1, 10)\n",
    "    \n",
    "    # Generate treatment with selection bias\n",
    "    treatment_logit = (\n",
    "        -2.0\n",
    "        - 0.03 * (age - 65)\n",
    "        - 0.2 * (severity - 5)\n",
    "        - 0.3 * prior_admits\n",
    "        + 0.8 * insurance\n",
    "        + 0.3 * (motivation - 5)\n",
    "    )\n",
    "    true_propensity = 1 / (1 + np.exp(-treatment_logit))\n",
    "    treatment = np.random.binomial(1, true_propensity)\n",
    "    \n",
    "    # Generate outcomes\n",
    "    baseline_logit = (\n",
    "        -1.5\n",
    "        + 0.02 * (age - 65)\n",
    "        + 0.25 * (severity - 5)\n",
    "        + 0.3 * prior_admits\n",
    "        - 0.15 * insurance\n",
    "        - 0.15 * (motivation - 5)\n",
    "    )\n",
    "    \n",
    "    prob_y0 = 1 / (1 + np.exp(-baseline_logit))\n",
    "    prob_y1 = 1 / (1 + np.exp(-(baseline_logit + true_treatment_effect * 5)))\n",
    "    \n",
    "    y0 = np.random.binomial(1, prob_y0)\n",
    "    y1 = np.random.binomial(1, prob_y1)\n",
    "    readmitted = np.where(treatment == 1, y1, y0)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'patient_id': range(1, n_patients + 1),\n",
    "        'age': np.round(age, 1),\n",
    "        'severity': np.round(severity, 2),\n",
    "        'prior_admits': prior_admits,\n",
    "        'insurance': insurance,\n",
    "        'motivation': np.round(motivation, 2),\n",
    "        'treatment': treatment,\n",
    "        'readmitted': readmitted,\n",
    "        '_true_propensity': true_propensity,\n",
    "        '_y0': y0,\n",
    "        '_y1': y1\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_cardiac_rehab_data(n_patients=2000)\n",
    "\n",
    "print(\"âœ… Dataset generated!\")\n",
    "print(f\"   Shape: {df.shape[0]} patients, {df.shape[1]} variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df679e33-bdb5-4a14-b1f0-f6e94a5e7f0f",
   "metadata": {},
   "source": [
    "### TODO 1: Explore the Data\n",
    "\n",
    "Complete the code below to explore the dataset and understand the selection bias problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c6488-2559-43ba-be82-13ae07c916b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 1: Explore the Data\n",
    "# =============================================================================\n",
    "\n",
    "# TODO 1.1: Display the first 5 rows of the dataset\n",
    "# Hint: Use df.head()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# TODO 1.2: Calculate the number of treated and control patients\n",
    "# Hint: Use df['treatment'].sum() for treated count\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "n_treated = ____\n",
    "n_control = ____\n",
    "\n",
    "print(f\"Treated patients: {n_treated}\")\n",
    "print(f\"Control patients: {n_control}\")\n",
    "\n",
    "\n",
    "# TODO 1.3: Calculate readmission rates for each group\n",
    "# Hint: Use df[df['treatment'] == 1]['readmitted'].mean() for treated group\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "readmit_treated = ____\n",
    "readmit_control = ____\n",
    "\n",
    "print(f\"Readmission rate (Treated): {readmit_treated*100:.1f}%\")\n",
    "print(f\"Readmission rate (Control): {readmit_control*100:.1f}%\")\n",
    "print(f\"Naive difference: {(readmit_treated - readmit_control)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751b83b-aaa9-476a-a792-2090074371d8",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output (approximately):\n",
    "```\n",
    "Treated patients: ~600-700\n",
    "Control patients: ~1300-1400\n",
    "Readmission rate (Treated): ~15-20%\n",
    "Readmission rate (Control): ~22-28%\n",
    "Naive difference: ~-5 to -10 percentage points\n",
    "```\n",
    "\n",
    "**â“ Question:** Why is the naive difference NOT the true causal effect?\n",
    "\n",
    "*Your answer:* _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c015143-a8a2-4565-aed7-1043b4a2c304",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TODO 2 Estimate Propensity Scores\n",
    "\n",
    "Now you'll build a logistic regression model to estimate each patient's probability of receiving treatment.\n",
    "\n",
    "**Covariates to use:** `age`, `severity`, `prior_admits`, `insurance`, `motivation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2ced-6da9-461e-82ed-f7a7d7a4199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 2: Estimate Propensity Scores\n",
    "# =============================================================================\n",
    "\n",
    "# Define covariates (provided for you)\n",
    "covariates = ['age', 'severity', 'prior_admits', 'insurance', 'motivation']\n",
    "\n",
    "# TODO 2.1: Prepare the features (X) and target (y)\n",
    "# Hint: X = df[covariates], y = df['treatment']\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "X = ____\n",
    "y = ____\n",
    "\n",
    "\n",
    "# TODO 2.2: Standardize the features using StandardScaler\n",
    "# Hint: Create a StandardScaler, then use fit_transform(X)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "scaler = ____\n",
    "X_scaled = ____\n",
    "\n",
    "\n",
    "# TODO 2.3: Fit a logistic regression model\n",
    "# Hint: Create LogisticRegression(max_iter=1000), then use .fit(X_scaled, y)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "model = ____\n",
    "____  # fit the model\n",
    "\n",
    "\n",
    "# TODO 2.4: Get propensity scores (predicted probabilities)\n",
    "# Hint: Use model.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "propensity_scores = ____\n",
    "\n",
    "\n",
    "# Add propensity scores to dataframe\n",
    "df['propensity_score'] = propensity_scores\n",
    "\n",
    "print(\"âœ… Propensity scores estimated!\")\n",
    "print(f\"   Min: {propensity_scores.min():.4f}\")\n",
    "print(f\"   Max: {propensity_scores.max():.4f}\")\n",
    "print(f\"   Mean: {propensity_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d252b-9137-4d01-b902-d5017221a6d8",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output (approximately):\n",
    "```\n",
    "âœ… Propensity scores estimated!\n",
    "   Min: ~0.05-0.10\n",
    "   Max: ~0.85-0.95\n",
    "   Mean: ~0.30-0.40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929533e5-e65a-4b3e-9177-21b7a8fb7b43",
   "metadata": {},
   "source": [
    "### Step 2: Visualize Propensity Score Overlap\n",
    "\n",
    "Check whether there's sufficient overlap between treated and control groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf1e90-59fd-44b8-a165-650f7723fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 3: Visualize Propensity Score Distributions\n",
    "# =============================================================================\n",
    "\n",
    "# TODO 3.1: Create a histogram showing PS distributions for both groups\n",
    "# Hint: \n",
    "#   - Use plt.hist() for each group\n",
    "#   - df[df['treatment'] == 0]['propensity_score'] gives control PS\n",
    "#   - df[df['treatment'] == 1]['propensity_score'] gives treated PS\n",
    "#   - Use alpha=0.6 for transparency, density=True for normalization\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# Plot control group histogram\n",
    "\n",
    "\n",
    "# Plot treated group histogram\n",
    "\n",
    "\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Propensity Score Distribution by Treatment Group')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO 3.2: Calculate the region of common support\n",
    "# Hint: Common support is from max(min of both groups) to min(max of both groups)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "ps_control = df[df['treatment'] == 0]['propensity_score']\n",
    "ps_treated = df[df['treatment'] == 1]['propensity_score']\n",
    "\n",
    "common_support_min = ____  # max of the two minimums\n",
    "common_support_max = ____  # min of the two maximums\n",
    "\n",
    "print(f\"Region of Common Support: [{common_support_min:.4f}, {common_support_max:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871fa07-a699-4acf-bd90-0cee89158177",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output:\n",
    "- Histogram should show two overlapping distributions\n",
    "- Common support region should span most of the PS range\n",
    "- If distributions don't overlap, PSM may not be appropriate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30062a-b962-493e-9377-9a1a18da425f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Perform Matching\n",
    "\n",
    "Now implement nearest neighbor matching to find comparable pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47233640-7862-4697-8608-ec0ba4c553a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 4: Implement Nearest Neighbor Matching\n",
    "# =============================================================================\n",
    "\n",
    "def nearest_neighbor_matching(df, propensity_col='propensity_score', \n",
    "                               treatment_col='treatment', caliper=None):\n",
    "    \"\"\"\n",
    "    Perform nearest neighbor matching on propensity scores.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with propensity scores\n",
    "    propensity_col : Name of propensity score column\n",
    "    treatment_col : Name of treatment column\n",
    "    caliper : Maximum allowed distance (None = no limit)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    matched_df : DataFrame with matched pairs\n",
    "    match_info : Dictionary with matching statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate treated and control\n",
    "    treated = df[df[treatment_col] == 1].copy()\n",
    "    control = df[df[treatment_col] == 0].copy()\n",
    "    \n",
    "    # Store matched pairs\n",
    "    matched_treated_idx = []\n",
    "    matched_control_idx = []\n",
    "    match_distances = []\n",
    "    \n",
    "    # Track available controls\n",
    "    available_control_idx = set(control.index.tolist())\n",
    "    unmatched_count = 0\n",
    "    \n",
    "    # TODO 4.1: For each treated unit, find the nearest control\n",
    "    # This loop is partially completed - fill in the missing parts\n",
    "    \n",
    "    for treated_idx in treated.index:\n",
    "        # Get propensity score of this treated unit\n",
    "        treated_ps = treated.loc[treated_idx, propensity_col]\n",
    "        \n",
    "        # Get available controls\n",
    "        available_controls = control.loc[list(available_control_idx)]\n",
    "        \n",
    "        if len(available_controls) == 0:\n",
    "            unmatched_count += 1\n",
    "            continue\n",
    "        \n",
    "        # TODO: Calculate distance to all available controls\n",
    "        # Hint: Use np.abs(available_controls[propensity_col] - treated_ps)\n",
    "        \n",
    "        # YOUR CODE HERE:\n",
    "        distances = ____\n",
    "        \n",
    "        \n",
    "        # TODO: Find the nearest neighbor (control with minimum distance)\n",
    "        # Hint: Use distances.idxmin() for index, distances.min() for value\n",
    "        \n",
    "        # YOUR CODE HERE:\n",
    "        nearest_idx = ____\n",
    "        nearest_distance = ____\n",
    "        \n",
    "        \n",
    "        # Check caliper (if specified)\n",
    "        if caliper is not None and nearest_distance > caliper:\n",
    "            unmatched_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Record the match\n",
    "        matched_treated_idx.append(treated_idx)\n",
    "        matched_control_idx.append(nearest_idx)\n",
    "        match_distances.append(nearest_distance)\n",
    "        \n",
    "        # Remove control from available pool (matching without replacement)\n",
    "        available_control_idx.discard(nearest_idx)\n",
    "    \n",
    "    # Create matched dataset\n",
    "    matched_treated = df.loc[matched_treated_idx].copy()\n",
    "    matched_treated['match_id'] = range(len(matched_treated_idx))\n",
    "    \n",
    "    matched_control = df.loc[matched_control_idx].copy()\n",
    "    matched_control['match_id'] = range(len(matched_control_idx))\n",
    "    \n",
    "    matched_df = pd.concat([matched_treated, matched_control], axis=0)\n",
    "    matched_df = matched_df.sort_values(['match_id', treatment_col], ascending=[True, False])\n",
    "    \n",
    "    # Compile statistics\n",
    "    match_info = {\n",
    "        'n_treated_original': len(treated),\n",
    "        'n_matched_pairs': len(matched_treated_idx),\n",
    "        'n_unmatched': unmatched_count,\n",
    "        'mean_distance': np.mean(match_distances) if match_distances else None\n",
    "    }\n",
    "    \n",
    "    return matched_df, match_info\n",
    "\n",
    "\n",
    "# TODO 4.2: Calculate caliper (0.2 Ã— standard deviation of propensity scores)\n",
    "# YOUR CODE HERE:\n",
    "caliper = ____\n",
    "\n",
    "print(f\"Caliper: {caliper:.4f}\")\n",
    "\n",
    "\n",
    "# TODO 4.3: Perform matching using the function above\n",
    "# YOUR CODE HERE:\n",
    "matched_df, match_info = ____\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nâœ… Matching completed!\")\n",
    "print(f\"   Matched pairs: {match_info['n_matched_pairs']}\")\n",
    "print(f\"   Unmatched treated: {match_info['n_unmatched']}\")\n",
    "print(f\"   Mean match distance: {match_info['mean_distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588ba24-2b1a-4562-bdb8-bac1fb2acc28",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output (approximately):\n",
    "```\n",
    "Caliper: ~0.03-0.05\n",
    "âœ… Matching completed!\n",
    "   Matched pairs: ~550-650\n",
    "   Unmatched treated: ~20-80\n",
    "   Mean match distance: ~0.005-0.015\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3f5ce-14dc-4fd6-8021-1bf04ebe837c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Assess Covariate Balance\n",
    "\n",
    "Check whether matching successfully balanced the confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd196c3-a4bb-4187-a645-69feedda5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 5: Calculate Standardized Mean Differences (SMD)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_smd(treated_values, control_values):\n",
    "    \"\"\"\n",
    "    Calculate Standardized Mean Difference.\n",
    "    \n",
    "    SMD = (mean_treated - mean_control) / pooled_std\n",
    "    \"\"\"\n",
    "    mean_treated = treated_values.mean()\n",
    "    mean_control = control_values.mean()\n",
    "    \n",
    "    # TODO 5.1: Calculate pooled standard deviation\n",
    "    # Formula: sqrt((var_treated + var_control) / 2)\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    var_treated = ____\n",
    "    var_control = ____\n",
    "    pooled_std = ____\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    smd = (mean_treated - mean_control) / pooled_std\n",
    "    return smd\n",
    "\n",
    "\n",
    "# TODO 5.2: Calculate SMD for each covariate before and after matching\n",
    "print(\"=\" * 60)\n",
    "print(\"COVARIATE BALANCE: Before vs After Matching\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Covariate':<15} {'SMD Before':<15} {'SMD After':<15} {'Status'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for covar in covariates:\n",
    "    # Before matching\n",
    "    treated_before = df[df['treatment'] == 1][covar]\n",
    "    control_before = df[df['treatment'] == 0][covar]\n",
    "    smd_before = calculate_smd(treated_before, control_before)\n",
    "    \n",
    "    # TODO: Calculate SMD after matching\n",
    "    # Hint: Use matched_df instead of df\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "    treated_after = ____\n",
    "    control_after = ____\n",
    "    smd_after = ____\n",
    "    \n",
    "    # Determine status\n",
    "    if abs(smd_after) < 0.1:\n",
    "        status = \"âœ… Excellent\"\n",
    "    elif abs(smd_after) < 0.25:\n",
    "        status = \"âš ï¸ Acceptable\"\n",
    "    else:\n",
    "        status = \"âŒ Poor\"\n",
    "    \n",
    "    print(f\"{covar:<15} {smd_before:<15.4f} {smd_after:<15.4f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6ba86-c722-4897-bdc5-ee27306da3d4",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output:\n",
    "- All SMD After values should be close to 0 (ideally < 0.1)\n",
    "- All covariates should show \"âœ… Excellent\" or \"âš ï¸ Acceptable\"\n",
    "- If any show \"âŒ Poor\", matching may need adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc521b-d8ef-479c-8b30-13374dc9ab2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Estimate Treatment Effect (ATT)\n",
    "\n",
    "Finally, estimate the Average Treatment Effect on the Treated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e539db-2907-47cd-b5d7-fc166348b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TODO 6: Estimate the Average Treatment Effect on the Treated (ATT)\n",
    "# =============================================================================\n",
    "\n",
    "# TODO 6.1: Calculate mean outcomes for treated and control in matched sample\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "mean_treated = ____  # Hint: matched_df[matched_df['treatment'] == 1]['readmitted'].mean()\n",
    "mean_control = ____  # Hint: matched_df[matched_df['treatment'] == 0]['readmitted'].mean()\n",
    "\n",
    "\n",
    "# TODO 6.2: Calculate ATT (difference in means)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "att = ____\n",
    "\n",
    "\n",
    "# TODO 6.3: Calculate standard error and 95% confidence interval\n",
    "n_pairs = match_info['n_matched_pairs']\n",
    "var_treated = matched_df[matched_df['treatment'] == 1]['readmitted'].var()\n",
    "var_control = matched_df[matched_df['treatment'] == 0]['readmitted'].var()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "se_att = ____  # Hint: np.sqrt((var_treated + var_control) / n_pairs)\n",
    "ci_lower = ____  # Hint: att - 1.96 * se_att\n",
    "ci_upper = ____  # Hint: att + 1.96 * se_att\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"AVERAGE TREATMENT EFFECT ON THE TREATED (ATT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nReadmission Rates:\")\n",
    "print(f\"   Treated: {mean_treated*100:.2f}%\")\n",
    "print(f\"   Control: {mean_control*100:.2f}%\")\n",
    "print(f\"\\nðŸŽ¯ ATT Estimate: {att*100:.2f} percentage points\")\n",
    "print(f\"   95% CI: [{ci_lower*100:.2f}%, {ci_upper*100:.2f}%]\")\n",
    "\n",
    "# Compare to naive estimate\n",
    "naive_att = df[df['treatment']==1]['readmitted'].mean() - df[df['treatment']==0]['readmitted'].mean()\n",
    "print(f\"\\nðŸ“Š Comparison:\")\n",
    "print(f\"   Naive estimate: {naive_att*100:.2f} pp\")\n",
    "print(f\"   PSM estimate:   {att*100:.2f} pp\")\n",
    "print(f\"   True effect:    ~-15 pp (built into data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719038bc-ce56-409c-b7ec-003109b2326b",
   "metadata": {},
   "source": [
    "#### âœ… Expected Output (approximately):\n",
    "```\n",
    "ðŸŽ¯ ATT Estimate: -10 to -15 percentage points\n",
    "   95% CI: Should not include 0 if effect is significant\n",
    "\n",
    "ðŸ“Š Comparison:\n",
    "   Naive estimate: ~-5 to -8 pp (biased toward 0)\n",
    "   PSM estimate:   ~-10 to -15 pp (closer to truth!)\n",
    "   True effect:    ~-15 pp\n",
    "```\n",
    "\n",
    "**ðŸŽ‰ Success!** Your PSM estimate should be much closer to the true effect than the naive estimate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c6517-4a58-44ba-aef4-90c2c0e30357",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary: What You Accomplished\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "1. âœ… Explored data and identified selection bias\n",
    "2. âœ… Estimated propensity scores using logistic regression\n",
    "3. âœ… Implemented nearest neighbor matching with caliper\n",
    "4. âœ… Assessed covariate balance using SMD\n",
    "5. âœ… Estimated the ATT and compared to naive estimate\n",
    "\n",
    "**Key Insight:** The naive estimate was biased because healthier patients self-selected into treatment. PSM corrected this by comparing treated patients to similar controls.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Reflection Questions\n",
    "\n",
    "1. Why was the naive estimate biased toward zero?\n",
    "\n",
    "2. What would happen if we had unmeasured confounders (e.g., family support)?\n",
    "\n",
    "3. How would you explain PSM to a non-technical stakeholder?\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "- Try different caliper values â€” how do results change?\n",
    "- Try matching WITH replacement â€” does ATT improve?\n",
    "- Review the complete tutorial for deeper understanding\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! You've successfully implemented Propensity Score Matching!** ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
